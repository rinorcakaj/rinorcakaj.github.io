---
title: "Spectral Batch Normalization: Normalization in the Frequency Domain"
collection: publications
permalink: /publication/2023-06-29-sbn
excerpt: 'Regularization is a set of techniques that are used to improve the generalization ability of deep neural networks. In this paper, we introduce spectral batch normalization (SBN), a novel effective method to improve generalization by normalizing feature maps in the frequency (spectral) domain.'
date: 2023-06-20
venue: 'IJCNN'
paperurl: 'https://arxiv.org/abs/2306.16999'
citation: 'Rinor Cakaj, Jens Mehnert, Bin Yang. &quot;Spectral Batch Normalization: Normalization in the Frequency Domain.&quot; <i>IJCNN 2023</i>.'
---

## Abstract
Regularization is a set of techniques that are used
to improve the generalization ability of deep neural networks.
In this paper, we introduce spectral batch normalization (SBN), a
novel effective method to improve generalization by normalizing
feature maps in the frequency (spectral) domain. The activations
of residual networks without batch normalization (BN) tend to
explode exponentially in the depth of the network at initialization.
This leads to extremely large feature map norms even though the
parameters are relatively small. These explosive dynamics can be
very detrimental to learning. BN makes weight decay regularization
on the scaling factors $\gamma, \beta$ approximately equivalent to an
additive penalty on the norm of the feature maps, which prevents
extremely large feature map norms to a certain degree. It was
previously shown that preventing explosive growth at the final
layer at initialization and during training in ResNets can recover
a large part of Batch Normalizationâ€™s generalization boost.
However, we show experimentally that, despite the approximate
additive penalty of BN, feature maps in deep neural networks
(DNNs) tend to explode at the beginning of the network and
that feature maps of DNNs contain large values during the
whole training. This phenomenon also occurs in a weakened
form in non-residual networks. Intuitively, it is not preferred
to have large values in feature maps since they have too much
influence on the prediction in contrast to other parts of the feature
map. SBN addresses large feature maps by normalizing them
in the frequency domain. In our experiments, we empirically
show that SBN prevents exploding feature maps at initialization
and large feature map values during the training. Moreover,
the normalization of feature maps in the frequency domain
leads to more uniform distributed frequency components. This
discourages the DNNs to rely on single frequency components
of feature maps. These, together with other effects (e.g. noise
injection, scaling and shifting of the feature map) of SBN, have
a regularizing effect on the training of residual and non-residual
networks. We show experimentally that using SBN in addition
to standard regularization methods improves the performance
of DNNs by a relevant margin, e.g. ResNet50 on CIFAR-100 by
2.31%, on ImageNet by 0.71% (from 76.80% to 77.51%) and
VGG19 on CIFAR-100 by 0.66%.

[\[Download paper here\]](/files/SN_IJCNN_2023.pdf)
[\[ArXiv\]](https://arxiv.org/abs/2306.16999)
